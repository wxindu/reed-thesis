## Literature yet to be reviewed

* * *

- **Title:** A Measure of Robustness to Misspecification
- **Author:** Susan Athey, Guido Imbens
- **Abstract:** Researchers often report estimates and standard errors for the object of interest (such as a treatment effect) based on a single specification of a statistical model. We propose a systematic approach to assessing sensitivity to specification. We construct estimates of the object of interest for each of a large set of models. Our proposed robustness measure is the standard deviation of the point estimates over the set of models. Each member of the set is generated by splitting the sample into two subsamples based on covariate values, constructing separate parameter estimates for each subsample, and then combining the results.
- **Link:** http://dx.doi.org/10.1257/aer.p20151020
- **Why care:** This is cited by the SCA paper. Seems relevant to the methods sampling from specifications.
- **Year:** 2014

* * *

- **Title:** Estimation and Accuracy After Model Selection
- **Author:** Bradley Efron
- **Abstract:** Classical statistical theory ignores model selection in assessing estimation accuracy. Here we consider bootstrap methods for computing standard errors and confidence intervals that take model selection into account. The methodology involves bagging, also known as bootstrap smoothing, to tame the erratic discontinuities of selection-based estimators. A useful new formula for the accuracy of bagging then provides standard errors for the smoothed estimators. Two examples, nonparametric and parametric, are carried through in detail: a regression model where the choice of degree (linear, quadratic, cubic, …) is determined by the Cp criterion and a Lasso-based estimation problem.
- **Link:** https://doi.org/10.1080/01621459.2013.823775
- **Why care:** This is cited by the SCA paper. Seems relevant. 
- **Year:** 2015

* * *

- **Title:** Why Most Published Research Findings Are False
- **Author:** John P. A. Ioannidis
- **Abstract:** There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.
- **Link:** https://doi.org/10.1371/journal.pmed.0020124
- **Why care:** Cited by the SCA paper. It may be relevant to the importance of methods like SCA that encounters researcher degrees of freedom. (seems like) it talks about what seem to be the factors that's affecting the quality of the studies (and results) and what we need to care about to avoid non-replicable results. 
- **Year:** 2005

* * *

- **Title:** I Just Ran Four Million Regressions
- **Author:** Xavier X. Sala-i-Martin
- **Abstract:** In this paper I try to move away from the Extreme Bounds method of identifying" Instead of analyzing the" extreme bounds of the estimates of the coefficient of a particular variable distribution. My claim in this paper is that, if we do this, the picture emerging from the" empirical growth literature is not the pessimistic Robust" that we get with the" extreme bound analysis. Instead, we find that a substantial number of variables can be found" to be strongly related to growth.
- **Link:** https://www.nber.org/papers/w6252.pdf
- **Why care:** The title looks appealing. Also seems to be relevant to the idea of fitting lots of models on one question
- **Year:** 1997

* * *

- **Title:** False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant
- **Author:** Joseph P. Simmons, Leif D. Nelson, Uri Simonsohn
- **Abstract:** In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.
- **Link:** https://doi.org/10.1177/0956797611417632
- **Why care:** Relevant to Researcher degrees of freedom, and more specifically in the psych field
- **Year:** 2011

* * *

- **Title:** The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant
- **Author:** Andrew Gelman & Hal Stern
- **Abstract:** It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary—for example, only a small change is required to move an estimate from a 5.1% significance level to 4.9%, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.
        
   The error we describe is conceptually different from other oft-cited problems—that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between “significant” and “not significant” is not itself statistically significant.
- **Link:** https://doi.org/10.1198/000313006X152649
- **Why care:** Seems relevant to problems of data analysis in scientific researches. 
- **Year:** 2012

* * *

- **Title:** A Memory-Based Model of Bounded Rationality*
- **Author:** Sendhil Mullainathan
- **Abstract:** In order to investigate the impact of limited memory on human behavior, I develop a model of memory grounded in psychological and biological research. I assume that people take their memories as accurate and use them to make inferences. The resulting model predicts both over- and underreaction but provides enough structure to predict when each effect dominates. I then use this framework to study the consumption decision. The results match empirical work on consumption predictability as well as differences in the marginal propensity to consume from different income streams. Most importantly, because it ties the extent of bias to a measurable aspect of the stochastic process being forecasted, the model makes testable empirical predictions.
- **Link:** https://doi.org/10.1162/003355302760193887
- **Why care:** Cited by the SCA paper. Seems relevant, "Most importantly, because it ties the extent of bias to a measurable aspect of the stochastic process being forecasted, the model makes testable empirical predictions."
- **Year:** 2002

* * *

- **Title:** Do not log‐transform count data
- **Author:** Robert B. O’Hara and D. Johan Kotze
- **Abstract:** 
    1. Ecological count data (e.g. number of individuals or species) are often log‐transformed to satisfy parametric test assumptions.

    2. Apart from the fact that generalized linear models are better suited in dealing with count data, a log‐transformation of counts has the additional quandary in how to deal with zero observations. With just one zero observation (if this observation represents a sampling unit), the whole data set needs to be fudged by adding a value (usually 1) before transformation.

    3. Simulating data from a negative binomial distribution, we compared the outcome of fitting models that were transformed in various ways (log, square root) with results from fitting models using quasi‐Poisson and negative binomial models to untransformed count data.

    4. We found that the transformations performed poorly, except when the dispersion was small and the mean counts were large. The quasi‐Poisson and negative binomial models consistently performed well, with little bias.

    5. We recommend that count data should not be analysed by log‐transforming it, but instead models based on Poisson and negative binomial distributions should be used.
- **Link:** https://doi.org/10.1111/j.2041-210X.2010.00021.x
- **Why care:** Cited by the SCA paper and looks cool
- **Year:** 2010

* * *
